---
title: "Homework 4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T,eval=T,message=F,warning=F,fig.align='center')
library(tidyverse)
```

## Problem 1. A Data Scientist Referees Ping Pong <small>(8pts total)</small>

The game is Ping Pong. Players grab their paddles and hit the ping pong ball back and forth scoring points one at a time. The game continues until one player reaches 21 points, and at that point the game ends unless the point difference is less than 2. If it is less than 2 the game continues until one player wins by 2. 

Suppose Athena and Bacchus play and Bacchus wins 21 to 15. Bacchus is super excited but Athena says that they should have a rematch, because she's sure that Bacchus is not better than her, it was just a fluke. 

Time for a Data Scientist to settle this dispute. We must consider two hypotheses. The null hypothesis is that they are equally skilled - thus for each point scored the probability it goes to the ultimate winner is $0.50$. This is basically what Athena is claiming. The alternative is that Bacchus truly is more skilled, and the probability is greater than $0.50$ (the winner actually has more skill, and doesn't win purely by chance).

Create a Monte Carlo simulation of a game. Use the point difference at game end as the test statistic. 

a. (4 points) Create a function called `playPingPong` which simulates a single game of Ping Pong with equally skilled players. Remember the logic : points are given to players A or B with equal chance, and that continues until (1) the max score >= 21 and (2) the difference between scores >=2. Have the function return the point difference.
b. (2 points) Perform the Monte Carlo test; simulate 1000 games with equally skilled players. Look at the distribution of point differences, and compare the observed point difference to this distribution. What is the p-value of the observed point difference.
c. (2 points) How do you conclude? Is this one game sufficient evidence that Bacchus is the superior Ping Pong Player?

```{r}
#a.
playPingPong <- function(scoreTrigger = 21, winMargin=2, probPlayerA=0.5){
  #initialize a score vector to be 0,0
  scores = c(0,0)
  #repeat while the winning condition has not been reached
  #i.e. repeat while max score < 21 or point difference < 2
  while(TRUE){
    player_win = sample(c(1,2), 1, prob = c(probPlayerA, 1 - probPlayerA))
    #Assign a point to player 1 or 2 randomly
    scores[player_win] = scores[player_win] + 1
    
    if(max(scores) >= 21 && abs(scores[1] - scores[2]) >= winMargin){
      break
    }
  }
  #return the point difference
  difference = abs(scores[1] - scores[2])
  return(difference)
}
```

```{r}
#b.
simulate = replicate(1000, playPingPong())
summary(simulate)
observedDiff = 6
pvalue = sum(simulate >= observedDiff) / length(simulate)
pvalue
```

c. I say that this isn't enough evidence that proves Bacchus is a better player because the pvalue came out as a 41% likelihood that they are both equally skilled meaning that we can't get rid of that null hypothesis and we would have to try and gather more evidence to prove that Bacchus is the superior player.


## Problem 2: Quality or Quantity? <small>(6pts total)</small> 

Marcio Ranchello (fictional) is a prolific architect who has won many accolades. For example, in the ranking of "Best 10 designs of 2023", 4 of the 10 designs are from Marcio Ranchello. The authors of the ranking report suggest that this is evidence of his greatness. However, you notice that among the 150 buildings considered in the rankings, 30 of them were designed by Marcio. Indeed, Marcio leads a big architecture firm that has been extremely active in designing new buildings.

What do you think? Is the ranking evidence of the quality of his work, or a consequence of the quantity of his designs?

Take the null hypothesis to be that any of the 150 considered buildings could be included in the top 10 with equal likelihood. How likely under this model would we see 4 (or more) of Ranchello's buildings in the top 10? What do you conclude? 

Proceed by treating this as a formal hypothesis test. Define the null and alternative hypotheses, define your test statistic, produce a distribution of simulated test statistics from the null model and finish by calculating a p-value and providing your own interpretation.

```{r}
buildings = c(rep(1,30), rep(0,120))

top_buildings <- function(){
  sum(sample(buildings, 10, replace = FALSE))
}

simulate = replicate(10000, top_buildings())
summary(simulate)
p_value = sum(simulate >= 4) / length(simulate)

p_value
```
After creating a Monte Carlo simulation with 10000 simulations of the 150 buildings, and dividing 30 to be Marcios and the rest 120, I got the p-value to see how many would be 4 or more from Marcios and it was an approximate 11% chance that this would happen meaning that there isn't sufficient evidence to reject the fact that Marcios Building are in the top 10 by chance. 

## Problem 3: Permutation testing <small>(8pts)</small>

Below are data arising from a (fictionalized) data source: the number of defects per day on an assembly line before and after installation of a new torque converter (this is a totally fictional "part" of an assembly line--just treat these as "control" and "treatment" groups, respectively).

```{r}
before = c(4,5,6,3,6,3,4,5,5,3,4,6,4,6,3,4,2,2,0,7,5,8,4,5,1,4,4,8,2,3)
after  = c(3,2,4,3,7,5,5,2,2,4,5,2,2,6,1,5,6,3,2,3,7,3,4,5,4,2,2,6,7,8)
```

a) (4 points) Use a permutation test to assess the claim that installation of the new part changed the prevalence of defects. That is, test the null hypothesis that the distribution of defects is the same before and after installation of the new part. Produce a p-value and interpret the results of your test in context.
```{r}
combined_data = c(before,after)
observed_stat = mean(after) - mean(before)

n = 0
p = 10000

for(i in 1:p){
  shuffled = sample(combined_data)
  #split
  new_before = shuffled[1:length(before)]
  new_after = shuffled[(length(before)+1):length(combined_data)]
  
  new_observed_stat = mean(new_after) - mean(new_before)
  
  if(abs(new_observed_stat) >= abs(observed_stat)){
    n = n + 1
  }
}
pvalue = n / p
pvalue

```
We can't reject this null-hypothesis because the p-value for this simulation of 10000 tries to see the defects for before and after came out to be as a 73% likelihood that the distribution for this stays the same. We would need more evidence to back up the other side since 73% is way too high to ignore.

b) (4 points) Explain, briefly, what you did above and why. Imagine that you are trying to explain to someone who isn't well versed in statistics what exactly you are doing in a permutation test. Explain your conclusion based on your test above. Three to five sentences should be plenty, but you are free to write as much or as little as you think is necessary to clearly explain your findings.
#I wanted to test the hypothesis to see if it's true that the installation of the new part has no effect on the number of defects. To test this I had to use a statistical method: permutation test which is basically getting numerical from 2 groups to compare. It helps us see if theres any observed differences in defect rates from before and after an installation is made. I needed to first set up my data before even doing this test to be able to get one combined group from before and after and get the observed data stat from it by calculating difference in mean. Then I made a function to actually start the test: I first had to shuffle the combined dataset and split it into 2 new groups of before and after to then get the difference in mean again as the new observed data stat, because when I do this I'm able to repeat this to get the distribution by keeping track of the count to how many times this new data is higher than the original one, that's how we keep track to be able to see the likelihood. Finally after this finishes running for how many ever times, I can just divide the count from new data being higher and divide it by the number of simulation (10000) and I got a likelihood of the hypothesis having 73% to happen so that means that we should get more evidence to try and back up the contrary hypothesis which would be that there is an effect on number of defects from before to after.


## Problem 4: Memes <small>(8pts)</small>

The following question comes from Karl Rohe, who developed the very first version of this class. This question has been reproduced in nearly the exact original (very amusing) wording.

> **Memes, part 1** (Please forgive me. I drank too much coffee before writing this question.)
> 
> In class thus far, there have been 416 comments posted in the bbcollaborate chat during class. An expert panel has judged 47 of these comments to be memes. The big-bad-deans say that they are concerned "if there is evidence that more than 10% of comments are memes." So, this looks like bad news, 47/416>10%.
> 
> Karl pleads with the deans: "Please, oh please, you big-bad-deans... Memeing is totally random." (I don't actually know what this notion of "random" means, but please just run with it for this question.) Then, along comes you, a trusty and dedicated 340 student. You say that "because we have only observed 416 comments, we don't really know what the 'true proportion' of memes."
> 
> 4a: What would be a good distribution for the number of memes?
> 
#We can use a binomial distribution because we can model the probability of observing the number of memes in a fixed number of the total comments where there's only 2 outputs- either it is a meme or not a meme.

> 4b: Using your distribution from 4a, test the null hypothesis that the 'true proportion' is actually 10%. It's all up to you now... report the p-value.

```{r}

n = 416
p = 0.1
observed = 47

pvalue = 1 - pbinom(observed - 1, n, p)
pvalue

```

Hints:

- For 4a, there should be a (hopefully) fairly intuitive choice of random variable that makes sense here. Look at your list of random variables and ask yourself which of these makes the most sense.
- For 4b, you can use the built-in function in R to simulate observations according to your null. Remember that you **always simulate *assuming* the null hypothesis**. Make sure your choice of the necessary parameter(s) reflects this assumption.

